# --- Start of SolarPlatform.py ---
import pgeocode
import numpy as np
from abc import ABC, abstractmethod
from zoneinfo import ZoneInfo
import streamlit as st
from typing import List, Dict
import diskcache
import pprint
from enum import Enum
from dataclasses import dataclass, field
from typing import Optional
import random
import math
from datetime import datetime, timedelta, time
import keyring
import api_keys

# Disk cache decorator to save remote API calls.
cache = diskcache.Cache(".")


@dataclass(frozen=True)
class BatteryInfo:
    serial_number: str
    model_name: str
    state_of_energy: str


@dataclass(frozen=True)
class AlertType:
    NO_COMMUNICATION = "NO_COMMUNICATION"
    PRODUCTION_ERROR = "PRODUCTION_ERROR"
    PANEL_ERROR = "PANEL_ERROR" # Microinverter or optimizer error
    CONFIG_ERROR = "CONFIG_ERROR"


@dataclass(frozen=True)
class SolarAlert:
    site_id: str
    alert_type: str
    severity: int  # severity in percentage (0-100% production down)
    details: str
    first_triggered: datetime

    def __post_init__(self):
        if not (0 <= self.severity <= 100):
            raise ValueError("Severity must be between 0 and 100.")


@dataclass(frozen=True)
class SiteInfo:
    site_id: str
    name: str
    url: str
    zipcode: str
    latitude: float
    longitude: float

def extract_vendor_code(site_id):
    if ':' in site_id:
        return site_id.split(':', 1)[0]
    else:
        raise ValueError(f"Invalid site_id: {site_id}. Expected a vendor code prefix + :")

# For each day, we store a set of ProductionRecord objects, one for each site.
@dataclass(frozen=True)
class ProductionRecord:
    site_id: str
    production_kw: List[float]

    # Two ProductionRecord objects are considered equal if they share the same vendor and site.
    def __hash__(self):
        return hash((self.site_id))

    def __eq__(self, other):
        if not isinstance(other, ProductionRecord):
            return NotImplemented
        return (self.site_id) == (other.site_id)

def calculate_production_kw(item):
    if isinstance(item, list):
        return sum(item)
    elif isinstance(item, (int, float)) and not np.isnan(item):
        return item
    else:
        return 0.0

class SolarPlatform(ABC):
    log_container = None # st.empty() A Streamlit container to display log messages
    log_text = ""  # A string to store cumulative log messages

    @classmethod
    @abstractmethod
    def get_vendorcode(cls):
        pass

    @classmethod
    @abstractmethod
    # Returns a dict of site_id (which contains a vendor code prefix) to SiteInfo objects
    def get_sites_map(cls) -> Dict[str, SiteInfo]:
        pass

    @classmethod
    @abstractmethod
    # returns a list of production for each inverter on site
    def get_production(cls, site_id, reference_time) -> List[float]:
        pass

    @classmethod
    @abstractmethod
    # returns a list of BatteryInfos for a site
    def get_batteries_soe(cls, site_id) -> List[BatteryInfo]:
        pass

    @classmethod
    @abstractmethod
    # Returns a list of SolarAlerts
    def get_alerts(cls) -> List[SolarAlert]:
        pass

    @classmethod
    def add_vendorcodeprefix(cls, site_id):
        return cls.get_vendorcode() + ":" + str(site_id)

    @classmethod
    def strip_vendorcodeprefix(cls, site_id):
        if ':' in site_id:
            site_id_raw = site_id.split(':', 1)[1]
            return site_id_raw
        else:
            return site_id
        
    @classmethod
    def log(cls, message: str):
        formatted_str = pprint.pformat(message, depth=None, width=120)
        print(formatted_str)
        current_logs = cache.get("global_logs", "")
        cache.set("global_logs", current_logs + formatted_str + "\n")


CACHE_EXPIRE_HOUR = 3600
CACHE_EXPIRE_DAY = CACHE_EXPIRE_HOUR * 24
CACHE_EXPIRE_WEEK = CACHE_EXPIRE_DAY * 7
CACHE_EXPIRE_YEAR = CACHE_EXPIRE_DAY * 365

# Scatter monthly requests over a period of 10 days to avoid cache stampede.


def CACHE_EXPIRE_MONTH():
    base = CACHE_EXPIRE_WEEK * 4
    offset = random.randint(-CACHE_EXPIRE_DAY * 5, CACHE_EXPIRE_DAY * 5)
    return base + offset


def disk_cache(expiration_seconds):
    def decorator(func):
        def wrapper(*args, **kwargs):
            cache_key = f"{func.__name__}_{args}_{kwargs}"
            if cache_key in cache:
                try:
                    return cache[cache_key]
                except KeyError:
                    pass
            result = func(*args, **kwargs)
            cache.set(cache_key, result, expire=expiration_seconds)
            return result
        return wrapper
    return decorator


# FIXME, harding codes Eastern timezone for now
def get_recent_noon(timezone_str="America/New_York") -> datetime:
    tz = ZoneInfo(timezone_str)  # Use specified timezone
    now = datetime.now(tz)
    today = now.date()

    threshold = datetime.combine(today, time(12, 30), tzinfo=tz)  # Threshold in specified tz

    measurement_date = today if now >= threshold else today - timedelta(days=1)

    noon_local = datetime.combine(measurement_date, time(12, 0), tzinfo=tz) # Noon in specified tz
    noon_utc = noon_local.astimezone(ZoneInfo("UTC")) # Convert to UTC

    return noon_utc

nomi = pgeocode.Nominatim('us')

def haversine_distance(lat1, lon1, lat2, lon2):

    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = math.sin(dlat/2)**2 + math.cos(lat1) * \
        math.cos(lat2) * math.sin(dlon/2)**2
    c = 2 * math.asin(math.sqrt(a))
    return c * 3958.8  # Earth radius in miles

@st.cache_data
def get_coordinates(zip_code):
    try:
        result = nomi.query_postal_code(zip_code)
        if result is None or math.isnan(result.latitude) or math.isnan(result.longitude):
            print(f"Failed to get coordinates for zip code: {zip_code}")
            result = nomi.query_postal_code(48071)
        return result.latitude, result.longitude
    except Exception as e:
        print(
            f"Exception thrown trying to get coordinates for zip code: {zip_code}")
        return 42.5, -83.1

def set_keyring_from_api_keys():
    """Sets API keys in the keyring based on variables in api_keys.py."""
    try:

        keyring.set_password("enphase", "client_id", api_keys.ENPHASE_CLIENT_ID)
        keyring.set_password("enphase", "client_secret", api_keys.ENPHASE_CLIENT_SECRET)
        keyring.set_password("enphase", "api_key", api_keys.ENPHASE_API_KEY)
        keyring.set_password("enphase", "user_email", api_keys.ENPHASE_USER_EMAIL)
        keyring.set_password("enphase", "user_password", api_keys.ENPHASE_USER_PASSWORD)

        # SolarEdge Keys (Storing individually)
        keyring.set_password("solaredge", "account_key", api_keys.SOLAREDGE_V2_ACCOUNT_KEY)
        keyring.set_password("solaredge", "api_key", api_keys.SOLAREDGE_V2_API_KEY)

        keyring.set_password("solark", "email", api_keys.SOLARK_EMAIL)
        keyring.set_password("solark", "password", api_keys.SOLARK_PASSWORD) 

        print("API keys set in keyring.")

    except AttributeError as e:
        print(f"Error: Missing API key in api_keys.py: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")


# If you want to display fake data for screenshots
FAKE_DATA = False


# --- Start of SqlModels.py ---
from datetime import datetime
from sqlalchemy import PrimaryKeyConstraint, create_engine, Column, String, Float, DateTime, Integer, Float, Date
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.types import PickleType

DATABASE_URL = "sqlite:///solar_alerts.db"
Base = declarative_base()
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(bind=engine)

#
# # Define tables

# from sqlalchemy import Index

# # For Alerts Table
# Index('idx_alerts_first_triggered', Alert.first_triggered)
# Index('idx_alerts_type', Alert.alert_type)

# # For ProductionHistory Table
# Index('idx_productionhistory_day', ProductionHistory.production_day)

# class Alert(Base):
#     __tablename__ = "alerts"
#     # ... existing columns ...
#     __table_args__ = (
#         Index('idx_alerts_first_triggered', 'first_triggered'),
#         Index('idx_alerts_type', 'alert_type'),
#     )

# class ProductionHistory(Base):
#     __tablename__ = "productionhistory"
#     # ... existing columns ...
#     __table_args__ = (
#         Index('idx_productionhistory_day', 'production_day'),
#     )


class Site(Base):
    __tablename__ = "sites"
    site_id = Column(String, nullable=False)
    name = Column(String, nullable=False)
    url = Column(String, nullable=False)

    history = Column(String, default="")  # Track alert history

    nearest_site_id = Column(String, nullable=False)
    nearest_distance = Column(String, nullable=False)

    __table_args__ = (
        PrimaryKeyConstraint('site_id'),
    )


class Alert(Base):
    __tablename__ = "alerts"
    site_id = Column(String, nullable=False)
    name = Column(String, nullable=False)
    url = Column(String, nullable=False)

    alert_type = Column(String, nullable=False)
    details = Column(String, nullable=False)
    severity = Column(Integer, nullable=False)

    first_triggered = Column(DateTime, nullable=False)
    resolved_date = Column(DateTime, nullable=True)

    __table_args__ = (
        PrimaryKeyConstraint('site_id', 'alert_type'),
    )


class ProductionHistory(Base):
    __tablename__ = "productionhistory"
    production_day = Column(Date, primary_key=True)
    # stores a set of SolarPlatform.ProductionRecord, one for each site
    data = Column(PickleType, nullable=False)
    total_noon_kw = Column(Float, nullable=False)


class Battery(Base):
    __tablename__ = "batteries"
    site_id = Column(String, nullable=False)
    serial_number = Column(String, nullable=False)

    model_number = Column(String, nullable=False)
    state_of_energy = Column(Float, nullable=True)

    last_updated = Column(DateTime, default=datetime.utcnow)

    __table_args__ = (
        PrimaryKeyConstraint('site_id', 'serial_number'),
    )


def init_fleet_db():
    Base.metadata.create_all(engine)


# --- Start of Database.py ---
from datetime import datetime, date
from typing import List, Tuple

import numpy as np
import pandas as pd
from sklearn.neighbors import BallTree

import SqlModels as Sql
import SolarPlatform


def add_site_if_not_exists(site_id, name, url, nearest_site_id, nearest_distance):
    session = Sql.SessionLocal()

    existing_site = session.query(Sql.Site).filter_by(site_id=site_id).first()

    if existing_site:
        session.close()
        return existing_site

    new_site = Sql.Site(
        site_id=site_id,
        name=name,
        url=url,
        history="Notes: ",
        nearest_site_id=nearest_site_id,
        nearest_distance=nearest_distance
    )
    session.add(new_site)
    session.commit()
    session.close()
    return new_site


def fetch_sites():
    session = Sql.SessionLocal()
    try:
        sites_df = pd.read_sql(session.query(Sql.Site).statement, session.bind)
        return sites_df
    finally:
        session.close()


def update_site_history(site_id, new_history):
    session = Sql.SessionLocal()
    try:
        site = session.query(Sql.Site).filter_by(site_id=site_id).first()
        if site:
            site.history = new_history
            session.commit()
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()


def add_alert_if_not_exists(site_id, name, url, alert_type, details, severity, first_triggered):
    with Sql.SessionLocal() as session:
        existing_alert = session.query(Sql.Alert).filter(
            Sql.Alert.site_id == site_id,
            Sql.Alert.alert_type == alert_type,
            # Check for unresolved alerts (i.e., NULL)
            Sql.Alert.resolved_date.is_(None)
        ).first()

        if not existing_alert:
            now = datetime.utcnow()

            new_alert = Sql.Alert(
                site_id=site_id,
                name=name,
                url=url,
                alert_type=str(alert_type),
                details=details,
                severity=severity,
                first_triggered=first_triggered,
                resolved_date=None,
            )
            session.add(new_alert)
            session.commit()


def update_battery_data(site_id, serial_number, model_number, state_of_energy):
    session = Sql.SessionLocal()

    existing_battery = session.query(Sql.Battery).filter(
        Sql.Battery.site_id == site_id,
        Sql.Battery.serial_number == serial_number
    ).first()

    if existing_battery:
        existing_battery.state_of_energy = state_of_energy
        existing_battery.last_updated = datetime.utcnow()
    else:
        new_battery = Sql.Battery(
            site_id=site_id,
            serial_number=serial_number,
            model_number=model_number,
            state_of_energy=state_of_energy,
            last_updated=datetime.utcnow()
        )
        session.add(new_battery)

    session.commit()
    session.close()

# Battery Data Update Function


def fetch_alerts():
    session = Sql.SessionLocal()

    query = session.query(Sql.Alert)
    alerts = pd.read_sql(query.statement, session.bind)
    session.close()
    return alerts


def delete_all_alerts():
    session = Sql.SessionLocal()
    try:
        session.query(Sql.Alert).delete()
        session.commit()
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()


def fetch_low_batteries():
    session = Sql.SessionLocal()
    query = session.query(Sql.Battery).filter(
        (Sql.Battery.state_of_energy < 0.10) | (Sql.Battery.state_of_energy.is_(None)))
    low_batteries = pd.read_sql(query.statement, session.bind)
    session.close()
    return low_batteries


def fetch_all_batteries():
    session = Sql.SessionLocal()
    query = session.query(Sql.Battery).order_by(
        Sql.Battery.state_of_energy.asc())
    all_batteries = pd.read_sql(query.statement, session.bind)
    session.close()
    return all_batteries

def get_total_noon_kw() -> pd.DataFrame:
    session = Sql.SessionLocal()
    try:
        results = session.query(
            Sql.ProductionHistory.production_day,
            Sql.ProductionHistory.total_noon_kw
        ).all()
        df = pd.DataFrame(results, columns=['production_day', 'total_noon_kw'])
        return df
    finally:
        session.close()


def get_production_set(production_day: datetime = None) -> set:
    session = Sql.SessionLocal()
    try:
        query = session.query(Sql.ProductionHistory)
        if production_day is None:
            record = query.order_by(Sql.ProductionHistory.production_day.desc()).first()
        else:
            production_date = production_day.date()
            record = query.filter_by(production_day=production_date).first()

        if record:
            return record.data
        return set()
    finally:
        session.close()


def insert_or_update_production_set(new_data: set[SolarPlatform.ProductionRecord], production_day):
    session = Sql.SessionLocal()
    try:
        # Retrieve the existing record by primary key using session.get()
        existing = session.get(Sql.ProductionHistory, production_day)

        if existing:
            combined_set = existing.data.copy()
            combined_set.update(new_data)
        else:
            # If no record exists, use a copy of new_data.
            combined_set = new_data.copy()

        # calculate the total noon production for all sites, to use for historical purposes.
        total_noon_kw = 0
        for site in combined_set:
            total_noon_kw += SolarPlatform.calculate_production_kw(site.production_kw)

        # Create a fresh instance with the merged data.
        new_record = Sql.ProductionHistory(production_day = production_day, data = combined_set, total_noon_kw = total_noon_kw)

        # session.merge() will check if a record with the given primary key exists;
        # if so, it will update that record with new_record‚Äôs state, otherwise it will add a new record.
        merged_record = session.merge(new_record)

        session.commit()
        return merged_record
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()


# Bulk process a list of SolarProduction data.

# Parameters:
#   - production_data: list of SolarProduction records.
#   - recalibrate: if True, override sanity checks (used when calibrating on a known sunny day).
#   - sunny_threshold: expected minimum average production on a sunny day.

# Behavior:
#   1. Computes the average production across all new records.
#      - If the average is below sunny_threshold and recalibrate is False, the function
#        refuses to update (to avoid cloudy-day corruption).
#   2. For each record, it checks if an entry (by vendor_code and site_id) already exists:
#      - If so, it updates the production and timestamp.
#      - Otherwise, it adds it as a new entry.
#   3. For new entries, it builds a combined dataset (existing + new) and uses a BallTree
#      (with the haversine metric) to compute for each new site the nearest neighbor‚Äôs vendor_code,
#      site_id, and distance (in miles). These values are stored in the new record.

    # # If there are new records, build a BallTree over the combined dataset.
    # if new_to_insert:
    #     coords = np.array([[r["lat"], r["lon"]] for r in combined])
    #     coords_rad = np.radians(coords)
    #     tree = BallTree(coords_rad, metric='haversine')
    #     keys = [(r["vendor_code"], r["site_id"]) for r in combined]

    #     # For each new record, compute the nearest neighbor.
    #     for rec in new_to_insert:
    #         new_key = (rec["vendor_code"], rec["site_id"])
    #         query_point = np.radians(np.array([[rec["lat"], rec["lon"]]]))
    #         dist_rad, ind = tree.query(query_point, k=2)
    #         # If the closest neighbor is itself, take the second closest.
    #         if keys[ind[0][0]] == new_key and len(ind[0]) > 1:
    #             nearest_idx = ind[0][1]
    #             distance_miles = dist_rad[0][1] * 3958.8
    #         else:
    #             nearest_idx = ind[0][0]
    #             distance_miles = dist_rad[0][0] * 3958.8
    #         rec["nearest_vendor_code"] = keys[nearest_idx][0]
    #         rec["nearest_site_id"] = keys[nearest_idx][1]
    #         rec["nearest_distance"] = int(round(distance_miles))


def process_bulk_solar_production(
        reference_date: date,
        production_data: set[SolarPlatform.ProductionRecord],
        recalibrate: bool = False,
        sunny_threshold: float = 100.0):

    if not production_data:
        print("No production data provided.")
        return

    # Compute average production for sanity check.
    production_kw = 0.0
    for site in production_data:
        production_kw += SolarPlatform.calculate_production_kw(site.production_kw)

    avg_prod = production_kw / len(production_data)
    # if avg_prod < sunny_threshold and not recalibrate:
    #     raise ValueError(
    #         f"Average production ({avg_prod:.2f}) is below the sunny threshold ({sunny_threshold}). "
    #         "Data rejected to prevent calibration on a cloudy day."
    #     )

    insert_or_update_production_set(production_data, reference_date)

    print(
        f"Processed {len(production_data)} records. Average production: {avg_prod:.2f}")


# --- Start of FleetCollector.py ---
from datetime import datetime, timedelta
from typing import List
import sys
import math
import time
import requests

from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
from webdriver_manager.chrome import ChromeDriverManager

import SqlModels as Sql
import SolarPlatform
import Database as db

from SolarEdge import SolarEdgePlatform
from Enphase import EnphasePlatform

def collect_platform(platform):
    sites = None
    platform.log("Starting collection at " + str(datetime.now()))
    production_set = set()
    reference_date = SolarPlatform.get_recent_noon()
    sites = platform.get_sites_map()

    try:
        for site_id in sites.keys():
            site = sites[site_id]
            # This needs to be moved to later when we have the nearest site information
            db.add_site_if_not_exists(site_id, sites[site_id].name, site.url, "nearest_siteid", "nearest_distance")

            battery_data = platform.get_batteries_soe(site_id)
            for battery in battery_data:
                db.update_battery_data(site_id, battery['serialNumber'], battery['model'], battery['stateOfEnergy'])

            # Fetch production data and put into set
            site_production_list = platform.get_production(site_id, reference_date)

            if site_production_list is not None:
                new_production = SolarPlatform.ProductionRecord(
                    site_id = site_id,
                    production_kw = site_production_list,
                )
                production_set.add(new_production)

        # Add production data to database
        db.process_bulk_solar_production(
            reference_date, production_set, False, 3.0)

    except Exception as e:
        platform.log(f"Error while fetching sites: {e}")
        return

    try:
        alerts = platform.get_alerts()
        for alert in alerts:
            db.add_alert_if_not_exists(alert.site_id, sites[alert.site_id].name, sites[alert.site_id].url, str(
                alert.alert_type), alert.details, alert.severity, alert.first_triggered)

    except Exception as e:
        platform.log(f"Error while fetching alerts: {e}")
        return


def run_collection():
    platformSolarEdge = SolarEdgePlatform()
    collect_platform(platformSolarEdge)
    
    platformEnphase = EnphasePlatform()
    collect_platform(platformEnphase)


# --- Start of Dashboard.py ---
from datetime import datetime, timedelta
from dataclasses import asdict
import requests
import numpy as np
import pandas as pd
import folium
import altair as alt
import yaml
from yaml.loader import SafeLoader

import streamlit as st
import streamlit.components.v1 as components
from streamlit_folium import st_folium
import streamlit_authenticator as stauth

import SolarPlatform
import SqlModels as Sql
import Database as db
from FleetCollector import collect_platform, run_collection
from SolarEdge import SolarEdgePlatform
from Enphase import EnphasePlatform

# to do:

#     dedicated logging library

# 4. Bulk Operations
#     Bulk Insert/Update for Production Data: 
#         session.bulk_save_objects(new_production_records)
#         session.commit()
# 5. Data Consistency and Sanity Checks

#     Sanity Checks in process_bulk_solar_production: 
#         Re-enable the commented-out sanity check to ensure you're not storing production data on cloudy days unless explicitly recalibrating:


# 6. Database Design

#     Materialized View for Historical Data: 
#         If you often summarize daily production, consider a materialized view for this purpose:

#         sql

#         CREATE MATERIALIZED VIEW daily_production_summary AS
#         SELECT production_day, SUM(total_noon_kw) as total_production
#         FROM productionhistory
#         GROUP BY production_day;


# 7. Error Handling and Logging

#     Improve error handling in database operations. 
# Use try-except blocks with specific exceptions where appropriate to handle and log errors more gracefully. 
# # Helper Functions


def send_browser_notification(title, message):
    js_code = f"""
    if ("Notification" in window) {{
        if (Notification.permission === "granted") {{
            new Notification("{title}", {{ body: "{message}" }}); 
        }} else if (Notification.permission !== "denied") {{
            Notification.requestPermission().then(permission => {{
                if (permission === "granted") {{
                    new Notification("{title}", {{ body: "{message}" }}); 
                }}
            }}); 
        }}
    }}
    """
    st.components.v1.html(f"<script>{js_code}</script>", height=0)

def format_production_tooltip(production_kw):
    if isinstance(production_kw, list):
        formatted_list = [f"{item:.2f}" for item in production_kw]
        return f"[{', '.join(formatted_list)}]"
    else:
        return f"{production_kw:.2f}"



def has_low_production(production):
    if isinstance(production, list):
        for production in production:
            if np.isnan(production) or production < 0.1:
                return True
        return False
    else: # Assume it's a single float
        production = production
        return np.isnan(production) or production < 0.1
    
def create_map_view(sites_df):
    # Center the map at the average location of all sites (initially)
    avg_lat = sites_df['latitude'].mean()
    avg_lon = sites_df['longitude'].mean()
    m = folium.Map(location=[avg_lat, avg_lon], zoom_start=5, width='100%')

    # Create a list to collect marker coordinates
    marker_coords = []

    # Define an approximate bounding box for Michigan.
    MIN_LAT, MAX_LAT = 41.7, 48.3
    MIN_LON, MAX_LON = -90, -82

    # Iterate over the DataFrame and add markers
    for _, row in sites_df.iterrows():
        lat = row['latitude']
        lon = row['longitude']

        # Sanity check: ignore if lat/lon is NaN or outside Michigan's bounding box.
        if np.isnan(lat) or np.isnan(lon) or lat < MIN_LAT or lat > MAX_LAT or lon < MIN_LON or lon > MAX_LON:
            print(
                f"Skipping marker for {row.get('site_id')} - {row.get('name')}: coordinates ({lat}, {lon}) out of bounds")
            continue

        marker_coords.append([lat, lon])

        # Check if any inverter is below the threshold
        if has_low_production(row["production_kw"]):
            color = "#FF0000"
        else:
            color = "#228B22"

        production_data = row["production_kw"] # Get production_kw for the current row

        # Format production_kw for the tooltip
        tooltip_content = format_production_tooltip(production_data)

        # Display the list of production values in the popup
        popup_html = (
            f"<strong>{row['name']} ({row['site_id']})</strong><br>"
            f"Production: {tooltip_content}"
        )

        total_production = SolarPlatform.calculate_production_kw(production_data)

        folium.Marker(
            location=[lat, lon],
            popup=folium.Popup(popup_html, max_width=300),
            icon=folium.DivIcon(
                html=f"""
                    <div style="
                        background-color: {color}; 
                        border-radius: 50%;
                        width: 30px;
                        height: 30px;
                        display: flex;
                        align-items: center;
                        justify-content: center;
                        color: white;
                        border: 2px solid #fff;
                        font-weight: bold;">
                        {total_production:.2f}
                    </div>
                """
            )
        ).add_to(m)

    if marker_coords:
        m.fit_bounds(marker_coords)

    st_folium(m, width=1200)


def display_historical_chart(historical_df):

    chart = alt.Chart(historical_df).mark_line(size=5).encode(
        x=alt.X('production_day:T', title='Date'),
        y=alt.Y('total_noon_kw:Q', title='Aggregated Production (KW)'),
        tooltip=['production_day:T', 'total_noon_kw:Q']
    ).properties(
        title="Historical Production Data"
    )

    st.altair_chart(chart, use_container_width=True)


def process_alert_section(df, header_title, editor_key, save_button_label, column_config, drop_columns=None, alert_type=None, use_container_width=True):
    st.header(header_title)
    if alert_type is not None:
        section_df = df[df['alert_type'] == alert_type].copy()
    else:
        section_df = df.copy()
      
    if drop_columns:
        section_df.drop(columns=drop_columns, inplace=True)

    edited_df = st.data_editor(
        section_df,
        key=editor_key,
        use_container_width=use_container_width,
        column_config=column_config
    )
    if st.button(save_button_label, key=editor_key + "_save"):
        for _, row in edited_df.iterrows():
            db.update_site_history(row['site_id'], row['history'])
    if alert_type is not None:
        df = df[df['alert_type'] != alert_type]
    return df

# Main Streamlit UI
title = "‚òÄÔ∏è AES Monitoring"
st.set_page_config(page_title=title, layout="wide")
Sql.init_fleet_db()
st.title(title)

with open('./config.yaml') as file:
    config = yaml.load(file, Loader=SafeLoader)

with open('./credentials.yaml') as file:
    credentials = yaml.load(file, Loader=SafeLoader)    

authenticator = stauth.Authenticate(
    credentials['credentials'],
    config['cookie']['name'],
    config['cookie']['key'],
    config['cookie']['expiry_days'],
)

if "authentication_status" not in st.session_state:
    st.session_state["authentication_status"] = None

try:
    auth_result = authenticator.login(location='main', key='Login')
except Exception as e:
    st.error(f"Login error: {e}")
    auth_result = None

if auth_result is not None:
    name, authentication_status, username = auth_result
else:
    authentication_status = st.session_state.get("authentication_status", None)

if authentication_status == True:
    authenticator.logout('Logout', 'main')
    
    #
    # After authentication
    #

    platform = SolarEdgePlatform()
    sites = platform.get_sites_map()

    platform = EnphasePlatform()
    sites_enphase = platform.get_sites_map()

    sites.update(sites_enphase)

    num_sites = len(sites)
    st.metric("Sites In Fleet", num_sites)

    st.header("üìä Historical Production Data")
    historical_df = db.get_total_noon_kw()
    display_historical_chart(historical_df)

    production_set = db.get_production_set(None)
    df_prod = pd.DataFrame([asdict(record) for record in production_set])

    with st.expander("Show Logs", expanded=False):
        st.text_area("Logs", value = SolarPlatform.cache.get("global_logs", ""), height=150)

    platform.log("Starting application at " + str(datetime.now()))

    # Create columns
    col1, col2, col3, col4, col5, col6 = st.columns(6)

    # Place buttons in columns
    with col1:
        if st.button("Run Data Collection"):
            run_collection()

    with col2:
        if st.button("Delete Alerts (Test)"):
            db.delete_all_alerts()
            st.success("All alerts deleted!")

    with col3:
        if st.button("Delete Alerts API Cache"):
            # Find cache keys that start with 'get_alerts'
            alerts_cache_keys = [
                key for key in SolarPlatform.cache.iterkeys()
                if key.startswith("get_alerts")
            ]
            # Delete each matching key from the cache
            for key in alerts_cache_keys:
                del SolarPlatform.cache[key]
            st.success("Alerts cache cleared!")

    with col4:
        if st.button("Delete Battery API Cache"):
            # Delete cache entries for battery data.
            battery_keys = [
                key
                for key in SolarPlatform.cache.iterkeys()
                if key.startswith("get_battery_state_of_energy")
            ]
            for key in battery_keys:
                del SolarPlatform.cache[key]
            st.success("Battery cache cleared!")
    with col5:
        if st.button("convert api_keys to keyring"):
            SolarPlatform.set_keyring_from_api_keys()
    with col6:
        if st.button("Clear Logs"):
            SolarPlatform.cache.delete("global_logs")


    st.markdown("---")

    st.header("üö® Active Alerts")

    alerts_df = db.fetch_alerts()
    alerts_df = alerts_df.drop(columns=["name", "url"], errors="ignore")

    # Generate synthetic alerts for sites with production below 0.1 kW
    existing_alert_sites = set(alerts_df['site_id'].unique())
    synthetic_alerts = []
    for record in production_set:
        if has_low_production(record.production_kw) and record.site_id not in existing_alert_sites:
            synthetic_alert = SolarPlatform.SolarAlert(
                site_id=record.site_id,
                alert_type=SolarPlatform.AlertType.PRODUCTION_ERROR,
                severity=100,
                details="",
                first_triggered=datetime.utcnow()
            )
            synthetic_alerts.append(synthetic_alert)

    if synthetic_alerts:
        synthetic_df = pd.DataFrame([asdict(alert) for alert in synthetic_alerts])
        alerts_df = pd.concat([alerts_df, synthetic_df], ignore_index=True)

    site_df = pd.DataFrame([asdict(site_info) for site_info in sites.values()])

    # Merge alerts_df with site_df to add 'name' and 'url'
    alerts_df = alerts_df.merge(site_df[['site_id', 'name', 'url']], on="site_id", how="left")

    alerts_df = alerts_df.drop(columns=["history"], errors="ignore")

    #Reorder columns
    alerts_df = alerts_df[['site_id', 'name', 'url'] + [col for col in alerts_df.columns if col not in ['site_id', 'name', 'url']]]

    # Fetch the site history
    sites_history_df = db.fetch_sites()[["site_id", "history"]]

    if not alerts_df.empty:
        # Merge site history once for all alerts
        merged_alerts_df = alerts_df.merge(
            sites_history_df, on="site_id", how="left")

        merged_alerts_df = process_alert_section(
            merged_alerts_df,
            header_title="Site Production failure",
            alert_type=SolarPlatform.AlertType.PRODUCTION_ERROR,
            editor_key="production_production",
            save_button_label="Save Production Site History Updates",
            column_config={
                "url": st.column_config.LinkColumn(label="Site url", display_text="Link")
            },
            drop_columns=["alert_type", "details", "resolved_date"],
        )

        merged_alerts_df = process_alert_section(
            merged_alerts_df,
            header_title="Site Communication failure",
            alert_type=SolarPlatform.AlertType.NO_COMMUNICATION,
            editor_key="comms_editor",
            save_button_label="Save Communication Site History Updates",
            column_config={
                "url": st.column_config.LinkColumn(label="Site url", display_text="Link"),
                "history": st.column_config.TextColumn(label="History                                                                                                     X")
            },
            drop_columns=["alert_type", "details", "severity"],
            use_container_width=False
        )

        merged_alerts_df = process_alert_section(
            merged_alerts_df,
            header_title="Panel-level failures",
            alert_type= SolarPlatform.AlertType.PANEL_ERROR,
            editor_key="panel_editor",
            save_button_label="Save Panel Site History Updates",
            column_config={
                "url": st.column_config.LinkColumn(label="Site url", display_text="Link")
            }
        )

        process_alert_section(
            merged_alerts_df,
            header_title="System Configuration failure",
            editor_key="sysconf_editor",
            save_button_label="Save System Config Site History Updates",
            column_config={
                "url": st.column_config.LinkColumn(label="Site url", display_text="Link")
            },
            alert_type=None
        )
    else:
        st.success("No active alerts.")

    st.header("üîã Batteries Below 10%")
    low_batteries_df = db.fetch_low_batteries()
    if not low_batteries_df.empty:
        # Merge battery info with site data to include 'name' and 'url'
        low_batteries_df = low_batteries_df.merge(
            site_df[['site_id', 'name', 'url']],
            on="site_id",
            how="left"
        )
        # Reorder columns: site_id, name, url first, then the rest.
        cols = low_batteries_df.columns.tolist()
        new_order = ['site_id', 'name', 'url'] + [c for c in cols if c not in ['site_id', 'name', 'url']]
        low_batteries_df = low_batteries_df[new_order]
        
        st.data_editor(
            low_batteries_df,
            key="low_batteries_editor",
            use_container_width=True,
            column_config={
                "url": st.column_config.LinkColumn(label="Site URL", display_text="Link")
            },
            disabled=True
        )
    else:
        st.success("All batteries above 10%.")

    with st.expander("üîã Full Battery List (Sorted by SOC, Hidden by Default)"):
        all_batteries_df = db.fetch_all_batteries()
        if all_batteries_df is not None and not all_batteries_df.empty:
            all_batteries_df = all_batteries_df.merge(
                site_df[['site_id', 'name', 'url']],
                on="site_id",
                how="left"
            )
            # Reorder columns: site_id, name, url first, then the rest.
            cols = all_batteries_df.columns.tolist()
            new_order = ['site_id', 'name', 'url'] + [c for c in cols if c not in ['site_id', 'name', 'url']]
            all_batteries_df = all_batteries_df[new_order]
            
            st.data_editor(
                all_batteries_df,
                key="all_batteries_editor",
                use_container_width=True,
                column_config={
                    "url": st.column_config.LinkColumn(label="Site URL", display_text="Link")
                },
                disabled=True
            )
        else:
            st.success("No battery data available.")

    st.header("üåç Site Map with Production Data")

    if not df_prod.empty and 'latitude' in site_df.columns:
        
        site_df["vendor_code"] = site_df["site_id"].apply(SolarPlatform.extract_vendor_code)
        site_df = site_df.merge(df_prod, on="site_id", how="left")

        site_df['production_kw_total'] = site_df['production_kw'].apply(SolarPlatform.calculate_production_kw)
        site_df['production_kw'] = site_df['production_kw'].round(2)

        create_map_view(site_df)

        st.markdown("---")    

        site_df.sort_values("production_kw_total", ascending=False, inplace=True)
        color_scale = alt.Scale(
            domain=["EN", "SE", "SMA", "Solis"],
            range=["orange", "#8B0000", "steelblue", "#A65E2E"]
        )

        chart = alt.Chart(site_df).mark_bar().encode(
            x=alt.X('production_kw_total:Q', title='Production (kW)'),
            y=alt.Y(
                'name:N',
                title='Site Name',
                sort=alt.SortField(field='production_kw_total', order='descending')
            ),
            color=alt.Color('vendor_code:N', scale=color_scale, title='Site Type'),
            tooltip=[
                alt.Tooltip('name:N', title='Site Name'),
                alt.Tooltip('production_kw_total:Q', title='Production (kW)')
            ]
        ).properties(
            title="Noon Production per Site",
            height=len(site_df) * 25
        )

        st.altair_chart(chart, use_container_width=True)
    else:
        st.info("No production data available.")


    st.dataframe(site_df)

elif authentication_status == False:
    st.error('Username/password is incorrect')
elif authentication_status == None:
    st.warning('Please enter your username and password')


# --- Start of SolarEdge.py ---
from dataclasses import dataclass
from typing import List, Dict
from datetime import datetime, timedelta
import numpy as np
import requests
import random
import streamlit as st
import keyring
import time
import api_keys
import SolarPlatform

SOLAREDGE_BASE_URL = 'https://monitoringapi.solaredge.com/v2'
SOLAREDGE_SITE_URL = 'https://monitoring.solaredge.com/solaredge-web/p/site/'

@dataclass(frozen=True)
class SolarEdgeKeys:
    account_key : str
    api_key : str

def fetch_solaredge_keys():
    try:
        account_key = keyring.get_password("solaredge", "account_key")
        api_key = keyring.get_password("solaredge", "api_key")

        if any(key is None for key in [account_key, api_key]):
            raise ValueError("Missing SolarEdge key(s) in keyring.")

        return SolarEdgeKeys(account_key, api_key)
    
    except Exception as e:
        print(f"Error fetching SolarEdge keys: {e}")
        return None
    
#SOLAREDGE_KEYS = fetch_solaredge_keys()

SOLAREDGE_SLEEP = 0.2

SOLAREDGE_KEYS = SolarEdgeKeys(api_keys.SOLAREDGE_V2_ACCOUNT_KEY, api_keys.SOLAREDGE_V2_API_KEY)

SOLAREDGE_HEADERS = {
        "X-API-Key": SOLAREDGE_KEYS.api_key, 
        "Accept": "application/json",
        "X-Account-Key": SOLAREDGE_KEYS.account_key,
    }

class SolarEdgePlatform(SolarPlatform.SolarPlatform):
    @classmethod
    def get_vendorcode(cls):
        return "SE"

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_WEEK)
    def get_sites_list(cls):
        url = f'{SOLAREDGE_BASE_URL}/sites'
        params = {"page": 1, "sites-in-page": 500}
        all_sites = []

        while True:
            cls.log("Fetching all sites from SolarEdge API...")
            response = requests.get(url, headers=SOLAREDGE_HEADERS, params=params)
            response.raise_for_status()
            sites = response.json()

            for site in sites:
                all_sites.append(site)

            if len(sites) < params["sites-in-page"]:
                break
            params["page"] += 1
        return all_sites

    @classmethod
    def get_sites_map(cls) -> Dict[str, SolarPlatform.SiteInfo]:
        sites = cls.get_sites_list()

        sites_dict = {}

        for site in sites:
            site_url = SOLAREDGE_SITE_URL + str(site.get('siteId'))
            site_id = cls.add_vendorcodeprefix(site.get('siteId'))
            zipcode = site['location']['zip']
            name = site.get('name')
            if SolarPlatform.FAKE_DATA:
                name = str(random.randint(1000, 9999)) + " Main St"

            latitude, longitude = SolarPlatform.get_coordinates(zipcode)
            site_info = SolarPlatform.SiteInfo(site_id, name, site_url, zipcode, latitude, longitude)
            sites_dict[site_id] = site_info

        return sites_dict

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_MONTH())
    def get_devices(cls, raw_site_id):
        url = f'{SOLAREDGE_BASE_URL}/sites/{raw_site_id}/devices'
        params = {"types": ["BATTERY", "INVERTER"]}

        cls.log(f"Fetching Inverter / battery inventory data from SolarEdge API for site {raw_site_id}.")
        time.sleep(SOLAREDGE_SLEEP)
        response = requests.get(url, headers=SOLAREDGE_HEADERS, params=params)
        response.raise_for_status()
        devices = response.json()
        return devices 
    

    @classmethod
    def get_inverters(cls, raw_site_id):
        devices = cls.get_devices(raw_site_id)

        inverters = [device for device in devices if device.get('type') == 'INVERTER']
        return inverters


    @classmethod
    def get_batteries(cls, raw_site_id):
        devices = cls.get_devices(raw_site_id)

        batteries = [device for device in devices if device.get('type') == 'BATTERY']
        return batteries


    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR * 2)
    def get_battery_state_of_energy(cls, raw_site_id, serial_number):
        end_time = datetime.utcnow()
        start_time = datetime.utcnow() - timedelta(minutes=15)

        url = f'{SOLAREDGE_BASE_URL}/sites/{raw_site_id}/storage/{serial_number}/state-of-energy'
        params = {'from': start_time.isoformat() + 'Z', 'to': end_time.isoformat() + 'Z',
                  'resolution': 'QUARTER_HOUR', 'unit': 'PERCENTAGE'}
        
        time.sleep(SOLAREDGE_SLEEP)
        cls.log(f"Fetching battery State of Energy from SolarEdge API for site {raw_site_id} and battery {serial_number}.")
        response = requests.get(url, headers=SOLAREDGE_HEADERS, params=params)
        response.raise_for_status()
        soe_data = response.json().get('values', [])

        latest_value = next((entry['value'] for entry in reversed(
            soe_data) if entry['value'] is not None), None)
        return latest_value

    @classmethod
    def get_batteries_soe(cls, site_id):
        raw_site_id = cls.strip_vendorcodeprefix(site_id)

        batteries = cls.get_batteries(raw_site_id)
        battery_states = []

        for battery in batteries:
            serial_number = battery.get('serialNumber')
            soe = cls.get_battery_state_of_energy(raw_site_id, serial_number)

            battery_states.append({'serialNumber': serial_number, 'model': battery.get(
                'model'), 'stateOfEnergy': soe})

        return battery_states

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_WEEK)
    def _get_inverter_production(cls, raw_site_id, reference_time, inverter_id):
        formatted_begin_time = reference_time.isoformat(timespec='seconds').replace('+00:00', 'Z')
        end_time = reference_time + timedelta(minutes=15)
        formatted_end_time = end_time.isoformat(timespec='seconds').replace('+00:00', 'Z')

        url = SOLAREDGE_BASE_URL + f'/sites/{raw_site_id}/inverters/{inverter_id}/power'
        params = {'from': formatted_begin_time , 'to': formatted_end_time,
                  'resolution': 'QUARTER_HOUR', 'unit': 'KW'}

        cls.log(f"Fetching production from SolarEdge API for site: {raw_site_id} inverter: {inverter_id} at {reference_time}.")
        time.sleep(SOLAREDGE_SLEEP)
        response = requests.get(url, headers=SOLAREDGE_HEADERS, params=params)
        response.raise_for_status()
        json = response.json().get('values', [])
        return json


    @classmethod
    def get_inverter_production(cls, raw_site_id, reference_time, inverter_id):
        powers = cls._get_inverter_production(raw_site_id, reference_time, inverter_id)
        power = powers[0].get('value', 0.0)
        if power is None:
            power = 0.0

        power = round(power, 2)
        return power


    @classmethod
    def get_production(cls, site_id, reference_time) -> List[float]:
        raw_site_id = cls.strip_vendorcodeprefix(site_id)
        inverters = cls.get_inverters(raw_site_id)

        productions = []
        for inverter in inverters:
            serial_number = inverter.get('serialNumber')
            power = cls.get_inverter_production(raw_site_id, reference_time, serial_number)
            productions.append(power)

        return productions
    
    @classmethod
    def convert_alert_to_standard(cls, alert):
        if alert == "SITE_COMMUNICATION_FAULT":
            return SolarPlatform.AlertType.NO_COMMUNICATION
        if alert == "INVERTER_BELOW_THRESHOLD_LIMIT":
            return SolarPlatform.AlertType.PRODUCTION_ERROR
        if alert == "PANEL_COMMUNICATION_FAULT":
            return SolarPlatform.AlertType.PANEL_ERROR
        else:
            return SolarPlatform.AlertType.CONFIG_ERROR

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR * 2)
    def get_alerts(cls) -> List[SolarPlatform.SolarAlert]:
        url = f'{SOLAREDGE_BASE_URL}/alerts'
        all_alerts = []

        try:
            response = requests.get(url, headers=SOLAREDGE_HEADERS)
            response.raise_for_status()
            alerts = response.json()
            for alert in alerts:
                site_id = cls.add_vendorcodeprefix(alert.get('siteId'))
                alert_details = ''  # FIXME
                first_triggered_str = alert.get('firstTrigger')
                # If the timestamp ends with a 'Z', replace it with '+00:00' for proper parsing
                if first_triggered_str and first_triggered_str.endswith("Z"):
                    first_triggered = datetime.fromisoformat(
                        first_triggered_str.replace("Z", "+00:00"))
                else:
                    first_triggered = first_triggered_str

                # Filter out unwanted alert types
                if alert.get('type') == 'SNOW_ON_SITE':
                    continue

                alert_type = cls.convert_alert_to_standard(alert.get('type'))
                solarAlert = SolarPlatform.SolarAlert(site_id, alert_type, alert.get('impact'), alert_details, first_triggered)
                all_alerts.append(solarAlert)

            return all_alerts
        except requests.exceptions.RequestException as e:
            print(f"Failed to retrieve SolarEdge alerts: {e}")
            return []


# --- Start of Enphase.py ---
from enum import Enum
import time
import requests
import base64
import keyring
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List

import SolarPlatform

ENPHASE_BASE_URL = "https://api.enphaseenergy.com"
ENPHASE_TOKENS = "Enphase Tokens"
ENPHASE_SITE_URL = "https://enlighten.enphaseenergy.com/systems/"


@dataclass(frozen=True)
class EnphaseKeys:
    client_id: str
    client_secret: str
    api_key: str
    user_email: str
    user_password: str

def fetch_enphase_keys():
    client_id = keyring.get_password("enphase", "client_id")
    client_secret = keyring.get_password("enphase", "client_secret")
    api_key = keyring.get_password("enphase", "api_key")
    user_email = keyring.get_password("enphase", "user_email")
    user_password = keyring.get_password("enphase", "user_password")

    if any(key is None for key in [client_id, client_secret, api_key, user_email, user_password]):
        raise ValueError("Missing Enphase key(s) in keyring.")

    return EnphaseKeys(client_id=client_id, client_secret=client_secret, api_key=api_key, user_email=user_email, user_password=user_password,
    )

import api_keys

#Enphase Installer API has 300 requests per minute, so add a small amount of sleeps to the API to prevent errors
#200 ms is probably overkill, since each request usually takes 1 second, but it is friendly to have some pauses.
ENPHASE_SLEEP = 0.2

ENPHASE_KEYS = EnphaseKeys(client_id=api_keys.ENPHASE_CLIENT_ID, client_secret=api_keys.ENPHASE_CLIENT_SECRET,
                            api_key=api_keys.ENPHASE_API_KEY, user_email=api_keys.ENPHASE_USER_EMAIL,
                            user_password=api_keys.ENPHASE_USER_PASSWORD)

#ENPHASE_KEYS = fetch_enphase_keys()

class EnphasePlatform(SolarPlatform.SolarPlatform):

    @classmethod
    def get_vendorcode(cls):
        return "EN"

    @staticmethod
    def get_basic_auth_header(client_id, client_secret):
        credentials = f"{client_id}:{client_secret}"
        encoded_credentials = base64.b64encode(credentials.encode()).decode()
        return {"Authorization": f"Basic {encoded_credentials}"}

    @classmethod
    def authenticate_enphase(cls, username, password, refresh_token=None):
        url = f"{ENPHASE_BASE_URL}/oauth/token"
        if refresh_token:
            data = {
                "grant_type": "refresh_token",
                "refresh_token": refresh_token
            }
        else:
            data = {
                "grant_type": "password",
                "username": username,
                "password": password
            }
        headers = EnphasePlatform.get_basic_auth_header(
            ENPHASE_KEYS.client_id, ENPHASE_KEYS.client_secret)
        try:
            response = requests.post(url, data=data, headers=headers)
            response.raise_for_status()
            tokens = response.json()
            expires_in = tokens.get("expires_in", 3600)
            return tokens.get("access_token"), tokens.get("refresh_token"), expires_in
        except requests.exceptions.RequestException as e:
            cls.log(f"Authentication failed: {e}")
            return None, None, None

    @classmethod
    def get_access_token(cls):
        current_time = int(time.time())
        tokens = SolarPlatform.cache.get(ENPHASE_TOKENS)
        if tokens:
            stored_access_token, stored_refresh_token, expires_at = tokens
            if current_time < expires_at:
                return stored_access_token
        # Try to authenticate
        access_token, new_refresh_token, expires_in = cls.authenticate_enphase(ENPHASE_KEYS.user_email, ENPHASE_KEYS.user_password)
        if access_token:
            expires_at = current_time + expires_in
            SolarPlatform.cache.set(ENPHASE_TOKENS, (access_token, new_refresh_token,
                                    expires_at), expire=SolarPlatform.CACHE_EXPIRE_YEAR)
            return access_token
        else:
            cls.log("Authentication failed in get_access_token")
            return None

    #We use this to check for alerts, so cache for a short period of time.
    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_DAY)
    def get_sites_list(cls) -> list:
        access_token = cls.get_access_token()
        if not access_token:
            return []

        all_systems = []
        page = 1
        size = 503  # Fetch many sites instead of the default 10.

        while True:
            url = f"{ENPHASE_BASE_URL}/api/v4/systems?key={ENPHASE_KEYS.api_key}&size={size}&page={page}"
            headers = {"Authorization": f"Bearer {access_token}"}
            try:
                response = requests.get(url, headers=headers)
                response.raise_for_status()
                raw_data = response.json()
                systems_page = raw_data.get("systems", [])
                all_systems.extend(systems_page)
                if len(systems_page) < size:
                    break
                page += 1
            except requests.exceptions.RequestException as e:
                cls.log(f"Failed to retrieve Enphase systems raw data for page {page}: {e}")
                break
        return all_systems

    @classmethod
    def get_sites_map(cls) -> Dict[str, SolarPlatform.SiteInfo]:
        raw_systems_data = cls.get_sites_list()
        sites_dict = {}
        for system in raw_systems_data:
            raw_system_id = system.get("system_id")
            site_id = cls.add_vendorcodeprefix(raw_system_id)
            name = system.get("name", f"System {raw_system_id}")
            location = system.get("location", {})
            zipcode = location.get("zip", "48071")
            latitude, longitude = SolarPlatform.get_coordinates(zipcode)
            site_url = ENPHASE_SITE_URL + str(raw_system_id)
            site_info = SolarPlatform.SiteInfo(site_id, name, site_url, zipcode, latitude, longitude)
            sites_dict[site_id] = site_info
        return sites_dict

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_WEEK)
    def get_production_micros(cls, site_id, reference_time) -> float:
        epoch_time = int(reference_time.timestamp())
        access_token = cls.get_access_token()
        if not access_token:
            return 0.0
        raw_system_id = cls.strip_vendorcodeprefix(site_id)
        url = f"{ENPHASE_BASE_URL}/api/v4/systems/{raw_system_id}/telemetry/production_micro?key={ENPHASE_KEYS.api_key}&start_at={epoch_time}&granularity=15mins"
        headers = {"Authorization": f"Bearer {access_token}"}
        try:
            time.sleep(ENPHASE_SLEEP)
            cls.log(f"Fetching production data from Enphase API for system {raw_system_id} at {reference_time}.")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            data = response.json()
            return data
        except requests.exceptions.RequestException as e:
            cls.log(
                f"Failed to retrieve production data for system {raw_system_id}: {e}")
            return 0.0

    #Enphase currently only has one production value per site.
    @classmethod
    def get_production(cls, site_id, reference_time) -> List[float]:
        json = cls.get_production_micros(site_id, reference_time)
        values = json.get("intervals", [])
        for entry in reversed(values):
            latest_value = entry.get("powr", 0.0)
            return [latest_value / 1000.0]
        return [0.0]

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_MONTH())
    def get_site_devices(cls, raw_system_id) -> dict:
        access_token = cls.get_access_token()
        if not access_token:
            return []
        url = f"{ENPHASE_BASE_URL}/api/v4/systems/{raw_system_id}/devices?key={ENPHASE_KEYS.api_key}"
        headers = {"Authorization": f"Bearer {access_token}"}
        try:
            time.sleep(ENPHASE_SLEEP)
            cls.log(f"Fetching devices from Enphase API for system {raw_system_id} (metadata).")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            json = response.json()
            return json
        except requests.exceptions.RequestException as e:
            cls.log(f"Failed to retrieve devices for system {raw_system_id}: {e}")
            return []

    @classmethod
    def get_batteries_metadata(cls, raw_system_id) -> list:
        json = cls.get_site_devices(raw_system_id)
        devices = json.get("devices", [])

        battery_devices = []
        for device in devices:
            if "encharges" in device:
                encharges = devices["encharges"]
                for encharge in encharges:
                    battery_devices.append(encharge)  # Add the device to the list

        return battery_devices

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_DAY)
    def get_battery_state_of_energy(cls, raw_system_id, serial_number):
        access_token = cls.get_access_token()
        if not access_token:
            return None
        url = f"{ENPHASE_BASE_URL}/api/v4/systems/{raw_system_id}/devices/encharges/{serial_number}/telemetry?key={ENPHASE_KEYS.api_key}"
        headers = {"Authorization": f"Bearer {access_token}"}
        try:
            time.sleep(ENPHASE_SLEEP)
            cls.log(f"Fetching battery telemetry for system {raw_system_id}, battery {serial_number}.")
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            json = response.json()
            return json
        except requests.exceptions.RequestException as e:
            cls.log(f"Failed to retrieve telemetry for battery {serial_number} in system {raw_system_id}: {e}")
            return None

    @classmethod
    def strip_trailing_digits(cls, s):
        i = len(s) - 1
        while s[i].isdigit():
            i -= 1
        return s[:i].strip()
    
    @classmethod
    def get_batteries_soe(cls, site_id) -> list:
        raw_system_id = cls.strip_vendorcodeprefix(site_id)
        batteries = cls.get_batteries_metadata(raw_system_id)
        battery_states = []
        for battery in batteries:
            serial_number = battery.get("serial_number")
            model = cls.strip_trailing_digits(battery.get("name"))
            json = cls.get_battery_state_of_energy(raw_system_id, serial_number)
            if json is None:
                soe = 0.0
            else:
                soe = json["intervals"][0]["soc"]["percent"]

            battery_states.append({
                "serialNumber": serial_number,
                "model": model,
                "stateOfEnergy": soe
            })
        return battery_states

    @classmethod
    def convert_alert_to_standard(cls, alert):
        if alert == "comm":
            return SolarPlatform.AlertType.NO_COMMUNICATION
        if alert == "power":
            return SolarPlatform.AlertType.PRODUCTION_ERROR
        if alert == "micro":
            return SolarPlatform.AlertType.PANEL_ERROR
        else:
            return SolarPlatform.AlertType.CONFIG_ERROR

    # get_sites_list() caches so use and adjust that one instead.
    @classmethod
    def get_alerts(cls) -> list:
        sites = cls.get_sites_list()
        alerts = []
        for site in sites:
            status = site.get("status", "normal").lower()
            if status != "normal":
                raw_system_id = site.get("system_id")
                site_id = cls.add_vendorcodeprefix(raw_system_id)
                alert_type = cls.convert_alert_to_standard(status)
                details = ""
                severity = 50  # FIXME Adjust severity
                 # FIXME Look for this data, at least for comms errors.
                first_triggered = datetime.utcnow()
                alert = SolarPlatform.SolarAlert(site_id, alert_type, severity, details, first_triggered)
                alerts.append(alert)
        return alerts
   


# --- Start of Solis.py ---
import aiohttp
import async_timeout
import asyncio
import time
import json
import hmac
import hashlib
from datetime import datetime, timedelta
from typing import Dict, List

import requests

import SolarPlatform

# API endpoints for Solis
RESOURCE_PREFIX = '/v1/api/'
USER_STATION_LIST = RESOURCE_PREFIX + 'userStationList'
STATION_DETAIL = RESOURCE_PREFIX + 'stationDetail'
COLLECTOR_LIST = RESOURCE_PREFIX + 'collectorList'
STATION_DAY = RESOURCE_PREFIX + 'stationDay'
ALARM_LIST = RESOURCE_PREFIX + 'alarmList'

# A base URL for constructing a site URL (for display purposes)
SOLIS_SITE_URL = "https://soliscloud.example.com/station/"

# Custom exception for Solis API errors
class SolisCloudAPIError(Exception):
    pass

class SolisCloudPlatform(SolarPlatform.SolarPlatform):
    _session: aiohttp.ClientSession = None

    @classmethod
    async def get_session(cls) -> aiohttp.ClientSession:
        if cls._session is None or cls._session.closed:
            cls._session = aiohttp.ClientSession()
        return cls._session

    @classmethod
    async def close_session(cls):
        if cls._session and not cls._session.closed:
            await cls._session.close()

    @classmethod
    def get_vendorcode(cls):
        return "SO"

    @classmethod
    async def _async_fetch_api_data(cls, endpoint: str, params: dict, ttl: int = SolarPlatform.CACHE_EXPIRE_HOUR) -> dict:
        # Replace with your actual API domain and any authentication logic as needed.
        domain = "https://api.soliscloud.example.com"
        url = domain.rstrip("/") + endpoint
        headers = {
            "Content-Type": "application/json"
        }
        session = await cls.get_session()
        try:
            async with async_timeout.timeout(10):
                async with session.post(url, json=params, headers=headers) as response:
                    if response.status != 200:
                        raise SolisCloudAPIError(f"HTTP error: {response.status}")
                    data = await response.json()
                    return data
        except asyncio.TimeoutError:
            raise SolisCloudAPIError("Timeout error occurred during API call")

    @classmethod
    def _fetch_api_data(cls, endpoint: str, params: dict, ttl: int = SolarPlatform.CACHE_EXPIRE_HOUR) -> dict:
        return asyncio.run(cls._async_fetch_api_data(endpoint, params, ttl))

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR)
    def get_user_station_list(cls, page_size: int = 100) -> dict:
        all_stations = []
        page_no = 1
        while True:
            params = {"pageNo": page_no, "pageSize": page_size}
            response = cls._fetch_api_data(USER_STATION_LIST, params)
            # Assuming the API returns the stations under a "data" key
            stations = response.get("data", [])
            if not stations:
                break
            all_stations.extend(stations)
            if len(stations) < page_size:
                break
            page_no += 1
        return {"data": all_stations}
    
    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR)
    def get_station_detail(cls, station_id: int) -> dict:
        params = {"id": station_id}
        return cls._fetch_api_data(STATION_DETAIL, params)

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR)
    def get_station_day(cls, station_id: int, time_str: str = None, currency: str = "USD", time_zone: int = 0) -> dict:
        if time_str is None:
            time_str = datetime.utcnow().strftime("%Y-%m-%d")
        params = {"money": currency, "time": time_str, "timeZone": time_zone, "id": station_id}
        return cls._fetch_api_data(STATION_DAY, params)

    @classmethod
    def process_station_data(cls, raw_data: dict) -> dict:
        # Transform the raw station list data into a dict with a "stations" key.
        processed = {"stations": raw_data.get("data", [])}
        return processed

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR)
    def get_sites_map(cls) -> Dict[str, SolarPlatform.SiteInfo]:
        # Fetch station list from the Solis API and map each station to a SiteInfo object.
        raw_data = cls.get_user_station_list(page_no=1, page_size=100)
        processed = cls.process_station_data(raw_data)
        stations_list = processed.get("stations", [])
        sites_dict = {}
        for station in stations_list:
            raw_station_id = station.get("id")
            if raw_station_id is None:
                continue
            site_id = cls.add_vendorcodeprefix(raw_station_id)
            name = station.get("name", "Unknown Station")
            url = SOLIS_SITE_URL + str(raw_station_id)
            zipcode = station.get("zipcode") or station.get("location", {}).get("zip", "48071")
            # Use SolarPlatform.get_coordinates to resolve latitude/longitude.
            latitude, longitude = SolarPlatform.get_coordinates(zipcode)
            site_info = SolarPlatform.SiteInfo(site_id, name, url, zipcode, latitude, longitude)
            sites_dict[site_id] = site_info
        return sites_dict

    @classmethod
    def get_production(cls, site_id, reference_time) -> List[float]:
        # Use the station day endpoint to fetch production data.
        raw_station_id = cls.strip_vendorcodeprefix(site_id)
        try:
            station_id_int = int(raw_station_id)
        except ValueError:
            station_id_int = raw_station_id
        date_str = reference_time.strftime("%Y-%m-%d")
        production_data = cls.get_station_day(station_id=station_id_int, time_str=date_str)
        # Assume the returned data includes an "inverters" list with power values.
        inverters = production_data.get("inverters", [])
        productions = []
        for inverter in inverters:
            power = inverter.get("power", 0.0)
            productions.append(round(power, 2))
        # Fallback if no inverter list exists.
        if not productions and "production" in production_data:
            productions = [round(production_data.get("production", 0.0), 2)]
        return productions

    @classmethod
    def get_batteries_soe(cls, site_id) -> List:
        # Fetch station detail and extract battery information.
        raw_station_id = cls.strip_vendorcodeprefix(site_id)
        try:
            station_id_int = int(raw_station_id)
        except ValueError:
            station_id_int = raw_station_id
        detail = cls.get_station_detail(station_id=station_id_int)
        batteries = detail.get("batteries", [])
        battery_states = []
        for battery in batteries:
            battery_states.append({
                "serialNumber": battery.get("serialNumber"),
                "model": battery.get("model", "Unknown"),
                "stateOfEnergy": battery.get("stateOfEnergy", 0.0)
            })
        return battery_states

    @classmethod
    @SolarPlatform.disk_cache(SolarPlatform.CACHE_EXPIRE_HOUR)
    def get_alerts(cls) -> List[SolarPlatform.SolarAlert]:
        alerts = []
        try:
            data = cls._fetch_api_data(ALARM_LIST, params={})
            alarms = data.get("alarms", [])
            for alarm in alarms:
                raw_station_id = alarm.get("siteId")
                if not raw_station_id:
                    continue
                site_id = cls.add_vendorcodeprefix(raw_station_id)
                alarm_type = alarm.get("type", "")
                # Map Solis-specific alarm types to the standard SolarPlatform.AlertType.
                if alarm_type == "COMM_FAULT":
                    alert_type = SolarPlatform.AlertType.NO_COMMUNICATION
                elif alarm_type == "PROD_ERROR":
                    alert_type = SolarPlatform.AlertType.PRODUCTION_ERROR
                elif alarm_type == "PANEL_ERROR":
                    alert_type = SolarPlatform.AlertType.PANEL_ERROR
                else:
                    alert_type = SolarPlatform.AlertType.CONFIG_ERROR
                severity = alarm.get("impact", 0)
                details = alarm.get("details", "")
                first_triggered_str = alarm.get("firstTrigger", "")
                if first_triggered_str and first_triggered_str.endswith("Z"):
                    first_triggered = datetime.fromisoformat(first_triggered_str.replace("Z", "+00:00"))
                else:
                    first_triggered = datetime.utcnow()
                alert_obj = SolarPlatform.SolarAlert(site_id, alert_type, severity, details, first_triggered)
                alerts.append(alert_obj)
        except Exception as e:
            cls.log(f"Error fetching alerts from Solis API: {e}")
        return alerts

# Example usage (for testing purposes)
if __name__ == "__main__":
    try:
        # Get and display station list
        raw_stations = SolisCloudPlatform.get_user_station_list(page_no=1, page_size=100)
        stations = SolisCloudPlatform.process_station_data(raw_stations)
        print("User Station List:")
        print(json.dumps(stations, indent=2))

        # Test fetching station detail, production, and alerts as needed.
    except Exception as e:
        print("Error during API call:", e)
    finally:
        asyncio.run(SolisCloudPlatform.close_session())




# Solar Fleet Monitoring Dashboard

![Dashboard Screenshot](https://github.com/user-attachments/assets/22b17fd1-0431-4813-90ca-c010ec9318f9)

A **Streamlit** dashboard for monitoring your solar fleet in real time. This project leverages intelligent caching, multiple solar API integrations, and even Selenium-based web scraping to ensure you always have the latest data while minimizing API costs.

---

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [API Support Details](#api-support-details)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgements](#acknowledgements)

---

## Overview

This dashboard is built with [Streamlit](https://streamlit.io/), a powerful yet simple UI framework for Python. It provides a centralized view for solar fleet monitoring by:

- **Caching API calls:** Dynamically caching responses from 1 hour to 1 month based on data change frequency to reduce unnecessary hits on production servers.
- **Integrating multiple APIs:** Currently supporting SolarEdge v2, Enphase v4 (partner API, nearly complete), and Sol-Ark via Selenium & web scraping, with more integrations on the way.
- **Flexible Data Retrieval:** Using Selenium to scrape data from web pages when no API is available.

---

## Features

- **Intelligent Caching:**
  - Automatically caches expensive API calls for durations ranging from one hour to one month.
  - Minimizes costs by reducing redundant calls to production servers during frequent code changes.
  
- **Multi-Source Data Integration:**
  - **SolarEdge v2 API:** Fully functional integration.
  - **Enphase v4 (Partner) API:** Integration is nearly complete.
  - **Sol-Ark:** Data extraction via Selenium (battery data is available; alerts are in progress).
  - Planned support for **Sonnen, SMA, AP-Smart**, and others.

- **Dynamic UI with Streamlit:**
  - Rapidly build interactive dashboards for data-centric projects.
  - Easily extendable and modifiable to suit different solar monitoring requirements.

---

## Installation

### Prerequisites

- Python 3.x
- [Streamlit](https://streamlit.io/)
- [Selenium](https://www.selenium.dev/) (only if using web scraping)
- Additional dependencies as listed in `requirements.txt`

### Setup Steps

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/yourusername/solar-fleet-dashboard.git
   cd solar-fleet-dashboard


2. **Clone the Repository:**

   ```bash
   pip install -r requirements.txt
   streamlit run Dashboard.py